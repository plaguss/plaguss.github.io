<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>A NER Model for Command Line Help Messages (Part 2: spaCy projects to the rescue) | Agus site</title><meta name=keywords content="NLP,NER,spaCy,Python"><meta name=description content="Lets continue our journey with helpner-core, the spaCy project that does all the magic behind helpner.
This repository contains a spaCy template for a NER model, and allows to build the end-to-end spaCy workflow from the spacy project cli command manager. In short, spaCy projects present End-to-end NLP workflows from prototype to production.
We will visit the different steps of the whole workflow following the commands that involve the pipeline."><meta name=author content><link rel=canonical href=https://plaguss.github.io/blog/a-ner-model-for-command-line-help-messages-part2/><link crossorigin=anonymous href=/assets/css/stylesheet.6a98292fb8fa8cf0f3ba4042d4b75515c04267550f3ad49ff6271b5af9562443.css integrity="sha256-apgpL7j6jPDzukBC1LdVFcBCZ1UPOtSf9icbWvlWJEM=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://plaguss.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://plaguss.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://plaguss.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://plaguss.github.io/apple-touch-icon.png><link rel=mask-icon href=https://plaguss.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://plaguss.github.io/blog/a-ner-model-for-command-line-help-messages-part2/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="A NER Model for Command Line Help Messages (Part 2: spaCy projects to the rescue)"><meta property="og:description" content="Lets continue our journey with helpner-core, the spaCy project that does all the magic behind helpner.
This repository contains a spaCy template for a NER model, and allows to build the end-to-end spaCy workflow from the spacy project cli command manager. In short, spaCy projects present End-to-end NLP workflows from prototype to production.
We will visit the different steps of the whole workflow following the commands that involve the pipeline."><meta property="og:type" content="article"><meta property="og:url" content="https://plaguss.github.io/blog/a-ner-model-for-command-line-help-messages-part2/"><meta property="article:section" content="blog"><meta property="article:published_time" content="2023-03-14T21:01:11+01:00"><meta property="article:modified_time" content="2023-03-14T21:01:11+01:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="A NER Model for Command Line Help Messages (Part 2: spaCy projects to the rescue)"><meta name=twitter:description content="Lets continue our journey with helpner-core, the spaCy project that does all the magic behind helpner.
This repository contains a spaCy template for a NER model, and allows to build the end-to-end spaCy workflow from the spacy project cli command manager. In short, spaCy projects present End-to-end NLP workflows from prototype to production.
We will visit the different steps of the whole workflow following the commands that involve the pipeline."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://plaguss.github.io/blog/"},{"@type":"ListItem","position":2,"name":"A NER Model for Command Line Help Messages (Part 2: spaCy projects to the rescue)","item":"https://plaguss.github.io/blog/a-ner-model-for-command-line-help-messages-part2/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"A NER Model for Command Line Help Messages (Part 2: spaCy projects to the rescue)","name":"A NER Model for Command Line Help Messages (Part 2: spaCy projects to the rescue)","description":"Lets continue our journey with helpner-core, the spaCy project that does all the magic behind helpner.\nThis repository contains a spaCy template for a NER model, and allows to build the end-to-end spaCy workflow from the spacy project cli command manager. In short, spaCy projects present End-to-end NLP workflows from prototype to production.\nWe will visit the different steps of the whole workflow following the commands that involve the pipeline.","keywords":["NLP","NER","spaCy","Python"],"articleBody":"Lets continue our journey with helpner-core, the spaCy project that does all the magic behind helpner.\nThis repository contains a spaCy template for a NER model, and allows to build the end-to-end spaCy workflow from the spacy project cli command manager. In short, spaCy projects present End-to-end NLP workflows from prototype to production.\nWe will visit the different steps of the whole workflow following the commands that involve the pipeline.\nhelpner-core workflow All this process is stored in the project.yml file, which contains the single source of truth for the project.\nDataset creation The first step is the dataset creation. I could have gone down the path of gathering a different number of CLI help messages and start labelling them, but it could easily get a very tedious task, and as the task allowed it, I started implementing a synthetic data generator (I will present cli-help-maker in a different post).\nLets introduce the first command:\nspacy project run create-dataset Running this command calls the program cli-help-maker, with a dataset.yaml file which contains the arguments needed for the program. This simplifies the versioning of the different data sets, we just have to store the yaml of any given version, all the relevant information to create a dataset is contained in a single file. Other than that the dataset contains random data with the layout and content we want the model to capture.\nThis command returns two files:\ndataset.jsonl: A jsonl file, where each line corresponds to a sample, that contains message and the annotations. {\"message\":\"Usage: \\n chedlock digynia borofluoric begirdle underbeadle [-d] [CONGRUENTLY...] [ACROBATISM... [ZONOID...]]\\n\\nFusillade preventively hogyard approachles.\\n\\nCommands:\\n digynia\\n...\", \"annotations\": [[\"CMD\",21,28],[\"CMD\", 29,40]...] } {\"message\": \"usage: \\n sterelmintha hearth --hydromorphic-beethovian -g --titanical -e ODONTOLCATE-\\n...\", \"annotations\":[[\"CMD\",25,31],[\"OPT\",32,57]...]} ... arguments.jsonl: Another jsonl file not used for during the program. We keep in this file the arguments that were used to generate each CLI help message, just for further analysis, to know what to expect from each dataset. {\"indent_spaces\":4.0,\"total_width\":120.0,...} {\"indent_spaces\":4.0,\"total_width\":78.0,...} Now we are ready to prepare the dataset for the training process.\nData workflow The process is divided in two different worflows just to outline the fact that the first part corresponds to data preparation, training and evaluation, while the next part deals with packaging related processes.\nData preparation This step involves two different scripts:\nsplit.py, which is run by the following program: spacy project run split This command simply splits the file in two different files dataset_train.jsonl and dataset_dev.jsonl with a random partition of 80/20. Given the dataset is generated randomly, there is no need for more complicated strategies.\nconvert.py, run by the following program: spacy project run convert It transforms the message in each row of the jsonl file to a serializable format: DocBin, to obtain two new files dataset_train.spacy and dataset_dev.spacy.\nIts time for some action!\nTraining The following program starts the training process:\nspacy project run train This command reads all the necessary info from the config files (you can read more in the spaCy training docs), which correspond to the NER components, CPU hardware and optimized for efficiency.\nIts created from the spaCy defaults, there is no need to change anything during this step. Given the model is just a proof of concept, and the dataset used is small enough, we can just train in our personal computer without a GPU, it just needs some minutes to finish the training process.\nAfter this step has finished, we are left with a directory containing our named entity recognition model.\nEvaluation After running the benchmark accuracy command we can inspect the model results on the development dataset. There should exist a different dataset to test the results, but for simplicity, the results are reported on this dataset, and the model is finally run against some help messages from real CLI apps (watch the helpner readme).\nspacy project run evaluate The output file of the command is a metrics.json file that contains information about the model‚Äôs performance (this content can be transferred to a README file easily as we will see later). The following table presents the information per entity, NER per type table:\nP R F CMD 98.25 99.96 99.10 ARG 94.79 89.97 92.32 OPT 98.88 98.96 98.92 The first thing to note is that the results seem to be really good for such a simple model with no special meaning besides the layout of the entities. The smallest value corresponds to the recall (R) value of ARG entities (89.97), the element that tends to get more confused, while the highest corresponds to the recall of CMD.\nBut, this results must be taken carefully, the dataset in which the model was trained is totaly synthetic, and the results may not be as good as expected in real data. It just shows that the model is able to learn from the data it was fed with.\nPackaging workflow Up to this point, we run the enabled workflows in project.yml file, which correspond to:\nspacy project run all Those are related to the data processing, model training and evaluation. A second workflow (more a subsequent set of commands) is for house keeping related tasks:\nPackaging The first of these commands is the package. SpaCy easily allows to generate a package automatically from our model, just by running a command, so it can be easily installed and loaded afterwards:\nspacy project run package Just place the contents generated somewhere accessible to pip, and you are ready to pip install the model to be used with spaCy. It even comes with a README.md file generated from the metrics.json file obtained, so the relevant content is autoexplained.\nFollowing the spaCy approach to model storage with spaCy models, the trained models are stored in the releases of the repository, and helpner deals with the installation process via pip install.\nReadme One last command and we are finished!\nSpaCy templates come with another magic command to generate a pretty README.md for your project: spacy project document. I wanted to add some more content automatically to this README file, and with a little script and the help of wasabi (a Explosion library which helps with console printing, but also with Markdown rendering), its as simple as it gets:\nspacy project run readme This command generates the readme for the helpner-core repository automatically, adding to the original generated README some additional metrics, like information from the dataset used for the current version, which seemed interesting enough to be added.\nDeployment The most relevant step make the model available for everybody easily isn‚Äôt properly mapped to a command, I have to do it manually for the moment üòÅ.\nBut lets assume the following command is already working (for the moment, this is done using GitHub in the browser directly):\nspacy project run release The previously created package is uploaded to github as a release (the releases can be seen in the following page), including a small description of the model‚Äôs accuracy, the weights and the necessary information to import the model just like any other spaCy model, after installation, it can be imported as usual in spaCy:\nimport spacy nlp = spacy.load(\"en_helpner_core\") And just like we would do with any other spaCy model, we can pass text to it. As te expected input for the model are command line help messages, helpner helps dealing with the content directly on the shell.\nConclusion In this post we followed workflow outlined in helpner-core. We leveraged the power of spaCy project template to train, evaluate, and deploy a Named Entity Recognition model which can be pip installed as a dependency to run as any spaCy model. Feels amazing to deal with an end to end NLP pipeline with just a few command line programs.\n","wordCount":"1278","inLanguage":"en","datePublished":"2023-03-14T21:01:11+01:00","dateModified":"2023-03-14T21:01:11+01:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://plaguss.github.io/blog/a-ner-model-for-command-line-help-messages-part2/"},"publisher":{"@type":"Organization","name":"Agus site","logo":{"@type":"ImageObject","url":"https://plaguss.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://plaguss.github.io/ accesskey=h title="Agus site (Alt + H)">Agus site</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li><li><a href=https://plaguss.github.io/es/ title=Espa√±ol aria-label=Espa√±ol>Es</a></li></ul></div></div><ul id=menu><li><a href=https://plaguss.github.io/ title=Blog><span>Blog</span></a></li><li><a href=https://plaguss.github.io/home/ title=Home><span>Home</span></a></li><li><a href=https://plaguss.github.io/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://plaguss.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>A NER Model for Command Line Help Messages (Part 2: spaCy projects to the rescue)</h1><div class=post-meta><span title='2023-03-14 21:01:11 +0100 +0100'>March 14, 2023</span>&nbsp;¬∑&nbsp;6 min</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#helpner-core-workflow aria-label="helpner-core workflow"><code>helpner-core</code> workflow</a><ul><ul><li><a href=#dataset-creation aria-label="Dataset creation">Dataset creation</a></li></ul><li><a href=#data-workflow aria-label="Data workflow">Data workflow</a><ul><li><a href=#data-preparation aria-label="Data preparation">Data preparation</a></li><li><a href=#training aria-label=Training>Training</a></li><li><a href=#evaluation aria-label=Evaluation>Evaluation</a></li></ul></li><li><a href=#packaging-workflow aria-label="Packaging workflow">Packaging workflow</a><ul><li><a href=#packaging aria-label=Packaging>Packaging</a></li><li><a href=#readme aria-label=Readme>Readme</a></li><li><a href=#deployment aria-label=Deployment>Deployment</a></li></ul></li></ul></li><li><a href=#conclusion aria-label=Conclusion>Conclusion</a></li></ul></div></details></div><div class=post-content><p>Lets continue our journey with <a href=https://github.com/plaguss/helpner-core>helpner-core</a>, the <a href=https://github.com/explosion/projects>spaCy project</a> that does all the magic behind <code>helpner</code>.</p><p><img loading=lazy src=/images/helpner-arch-part2.png alt=helpner-core></p><p>This repository contains a spaCy template for a NER model, and allows to build the end-to-end spaCy workflow from the <a href=https://spacy.io/api/cli#project><code>spacy project</code></a> cli command manager. In short, spaCy projects present <em>End-to-end NLP workflows from prototype to production</em>.</p><p>We will visit the different steps of the whole workflow following the commands that involve the pipeline.</p><h2 id=helpner-core-workflow><code>helpner-core</code> workflow<a hidden class=anchor aria-hidden=true href=#helpner-core-workflow>#</a></h2><p>All this process is stored in the <a href=https://github.com/plaguss/helpner-core/blob/main/project.yml><code>project.yml</code></a> file, which contains the single source of truth for the project.</p><h4 id=dataset-creation>Dataset creation<a hidden class=anchor aria-hidden=true href=#dataset-creation>#</a></h4><p><img loading=lazy src=/images/helpner2/workflow_1.png alt=helpner-core></p><p>The first step is the dataset creation. I could have gone down the path of gathering a different number of CLI help messages and start labelling them, but it could easily get a very tedious task, and as the task allowed it, I started implementing a synthetic data generator (I will present <a href=https://github.com/plaguss/cli-help-maker>cli-help-maker</a> in a different post).</p><p>Lets introduce the first command:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-console data-lang=console><span class=line><span class=cl><span class=go>spacy project run create-dataset
</span></span></span></code></pre></div><p>Running this command calls the program <a href=https://github.com/plaguss/cli-help-maker>cli-help-maker</a>, with a <a href=https://github.com/plaguss/helpner-core/blob/main/assets/v0.2.0/dataset.yaml><code>dataset.yaml</code></a> file which contains the arguments needed for the program. This simplifies the versioning of the different data sets, we just have to store the <em>yaml</em> of any given version, all the relevant information to create a dataset is contained in a single file. Other than that the dataset contains random data with the layout and content we want the model to capture.</p><p>This command returns two files:</p><ul><li><code>dataset.jsonl</code>: A <a href=https://jsonlines.org/>jsonl</a> file, where each line corresponds to a sample, that contains <em>message</em> and the <em>annotations</em>.</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-json data-lang=json><span class=line><span class=cl><span class=p>{</span><span class=nt>&#34;message&#34;</span><span class=p>:</span><span class=s2>&#34;Usage: \n    chedlock digynia borofluoric begirdle underbeadle [-d] [CONGRUENTLY...] [ACROBATISM... [ZONOID...]]\n\nFusillade preventively hogyard approachles.\n\nCommands:\n    digynia\n...&#34;</span><span class=p>,</span> <span class=nt>&#34;annotations&#34;</span><span class=p>:</span> <span class=p>[[</span><span class=s2>&#34;CMD&#34;</span><span class=p>,</span><span class=mi>21</span><span class=p>,</span><span class=mi>28</span><span class=p>],[</span><span class=s2>&#34;CMD&#34;</span><span class=p>,</span> <span class=mi>29</span><span class=p>,</span><span class=mi>40</span><span class=p>]</span><span class=err>...</span><span class=p>]</span> <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>{</span><span class=nt>&#34;message&#34;</span><span class=p>:</span> <span class=s2>&#34;usage: \n    sterelmintha hearth --hydromorphic-beethovian -g --titanical -e ODONTOLCATE-\n...&#34;</span><span class=p>,</span> <span class=nt>&#34;annotations&#34;</span><span class=p>:[[</span><span class=s2>&#34;CMD&#34;</span><span class=p>,</span><span class=mi>25</span><span class=p>,</span><span class=mi>31</span><span class=p>],[</span><span class=s2>&#34;OPT&#34;</span><span class=p>,</span><span class=mi>32</span><span class=p>,</span><span class=mi>57</span><span class=p>]</span><span class=err>...</span><span class=p>]}</span>
</span></span><span class=line><span class=cl><span class=err>...</span>
</span></span></code></pre></div><ul><li><code>arguments.jsonl</code>: Another <code>jsonl</code> file not used for during the program. We keep in this file the arguments that were used to generate each CLI help message, just for further analysis, to know what to expect from each dataset.</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-json data-lang=json><span class=line><span class=cl><span class=p>{</span><span class=nt>&#34;indent_spaces&#34;</span><span class=p>:</span><span class=mf>4.0</span><span class=p>,</span><span class=nt>&#34;total_width&#34;</span><span class=p>:</span><span class=mf>120.0</span><span class=p>,</span><span class=err>...</span><span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>{</span><span class=nt>&#34;indent_spaces&#34;</span><span class=p>:</span><span class=mf>4.0</span><span class=p>,</span><span class=nt>&#34;total_width&#34;</span><span class=p>:</span><span class=mf>78.0</span><span class=p>,</span><span class=err>...</span><span class=p>}</span>
</span></span></code></pre></div><p>Now we are ready to prepare the dataset for the training process.</p><h3 id=data-workflow>Data workflow<a hidden class=anchor aria-hidden=true href=#data-workflow>#</a></h3><p>The process is divided in two different worflows just to outline the fact that the first part corresponds to data preparation, training and evaluation, while the next part deals with packaging related processes.</p><h4 id=data-preparation>Data preparation<a hidden class=anchor aria-hidden=true href=#data-preparation>#</a></h4><p><img loading=lazy src=/images/helpner2/workflow_2.png alt=helpner-core></p><p>This step involves two different scripts:</p><ul><li><a href=https://github.com/plaguss/helpner-core/blob/main/scripts/split.py><code>split.py</code></a>, which is run by the following program:</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-console data-lang=console><span class=line><span class=cl><span class=go>spacy project run split
</span></span></span></code></pre></div><p>This command simply splits the file in two different files <code>dataset_train.jsonl</code> and <code>dataset_dev.jsonl</code> with a random partition of 80/20. Given the dataset is generated randomly, there is no need for more complicated strategies.</p><ul><li><a href=https://github.com/plaguss/helpner-core/blob/main/scripts/convert.py><code>convert.py</code></a>, run by the following program:</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-console data-lang=console><span class=line><span class=cl><span class=go>spacy project run convert
</span></span></span></code></pre></div><p>It transforms the <em>message</em> in each row of the <code>jsonl</code> file to a serializable format: <a href=https://spacy.io/api/docbin/><code>DocBin</code></a>, to obtain two new files <code>dataset_train.spacy</code> and <code>dataset_dev.spacy</code>.</p><p>Its time for some action!</p><h4 id=training>Training<a hidden class=anchor aria-hidden=true href=#training>#</a></h4><p><img loading=lazy src=/images/helpner2/workflow_3.png alt=helpner-core></p><p>The following program starts the training process:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-console data-lang=console><span class=line><span class=cl><span class=go>spacy project run train
</span></span></span></code></pre></div><p>This command reads all the necessary info from the <a href=https://github.com/plaguss/helpner-core/tree/main/configs>config files</a> (you can read more in the spaCy <a href=https://spacy.io/usage/training#quickstart>training docs</a>), which correspond to the NER <em>components</em>, CPU <em>hardware</em> and <em>optimized for</em> efficiency.</p><p>Its created from the spaCy defaults, there is no need to change anything during this step. Given the model is just a proof of concept, and the dataset used is small enough, we can just train in our personal computer without a GPU, it just needs some minutes to finish the training process.</p><p>After this step has finished, we are left with a directory containing our named entity recognition model.</p><h4 id=evaluation>Evaluation<a hidden class=anchor aria-hidden=true href=#evaluation>#</a></h4><p><img loading=lazy src=/images/helpner2/workflow_4.png alt=helpner-core></p><p>After running the <a href=https://spacy.io/api/cli#benchmark-accuracy><code>benchmark accuracy</code></a> command we can inspect the model results on the <em>development</em> dataset. There should exist a different dataset to test the results, but for simplicity, the results are reported on this dataset, and the model is finally run against some help messages from real CLI apps (watch the <code>helpner</code> readme).</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-console data-lang=console><span class=line><span class=cl><span class=go>spacy project run evaluate
</span></span></span></code></pre></div><p>The output file of the command is a <code>metrics.json</code> file that contains information about the model&rsquo;s performance (this content can be transferred to a README file easily as we will see later). The following table presents the information per entity, <strong>NER per type table:</strong></p><table><thead><tr><th></th><th>P</th><th>R</th><th>F</th></tr></thead><tbody><tr><td>CMD</td><td>98.25</td><td>99.96</td><td>99.10</td></tr><tr><td>ARG</td><td>94.79</td><td>89.97</td><td>92.32</td></tr><tr><td>OPT</td><td>98.88</td><td>98.96</td><td>98.92</td></tr></tbody></table><p>The first thing to note is that the results seem to be really good for such a simple model with no special meaning besides the layout of the entities. The smallest value corresponds to the <em>recall</em> (R) value of <code>ARG</code> entities (89.97), the element that tends to get more confused, while the highest corresponds to the <em>recall</em> of <code>CMD</code>.</p><p><em>But</em>, this results must be taken carefully, the dataset in which the model was trained is totaly synthetic, and the results may not be as good as expected in real data. <em>It just shows that the model is able to learn from the data it was fed with.</em></p><h3 id=packaging-workflow>Packaging workflow<a hidden class=anchor aria-hidden=true href=#packaging-workflow>#</a></h3><p>Up to this point, we run the enabled workflows in <code>project.yml</code> file, which correspond to:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-console data-lang=console><span class=line><span class=cl><span class=go>spacy project run all
</span></span></span></code></pre></div><p>Those are related to the data processing, model training and evaluation. A second workflow (more a subsequent set of commands) is for house keeping related tasks:</p><h4 id=packaging>Packaging<a hidden class=anchor aria-hidden=true href=#packaging>#</a></h4><p><img loading=lazy src=/images/helpner2/workflow_5.png alt=helpner-core></p><p>The first of these commands is the <a href=https://spacy.io/api/cli#package><code>package</code></a>. SpaCy easily allows to generate a package automatically from our model, just by running a command, so it can be easily installed and loaded afterwards:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-console data-lang=console><span class=line><span class=cl><span class=go>spacy project run package
</span></span></span></code></pre></div><p>Just place the contents generated somewhere accessible to pip, and you are ready to <code>pip install</code> the model to be used with spaCy. It even comes with a README.md file generated from the metrics.json file obtained, so the relevant content is autoexplained.</p><p>Following the spaCy approach to model storage with <a href=https://github.com/explosion/spacy-models>spaCy models</a>, the trained models are stored in the <a href=https://github.com/plaguss/helpner-core/releases>releases</a> of the repository, and <code>helpner</code> deals with the installation process via <code>pip install</code>.</p><h4 id=readme>Readme<a hidden class=anchor aria-hidden=true href=#readme>#</a></h4><p><img loading=lazy src=/images/helpner2/workflow_6.png alt=helpner-core></p><p>One last command and we are finished!</p><p>SpaCy templates come with another magic command to generate a pretty README.md for your project: <a href=https://spacy.io/api/cli#project-document><code>spacy project document</code></a>. I wanted to add some more content automatically to this README file, and with a little script and the help of <a href=https://github.com/explosion/wasabi><code>wasabi</code></a> (a Explosion library which helps with console printing, but also with Markdown rendering), its as simple as it gets:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-console data-lang=console><span class=line><span class=cl><span class=go>spacy project run readme
</span></span></span></code></pre></div><p>This command generates the readme for the <code>helpner-core</code> repository automatically, adding to the original generated README some additional metrics, like information from the dataset used for the current version, which seemed interesting enough to be added.</p><h4 id=deployment>Deployment<a hidden class=anchor aria-hidden=true href=#deployment>#</a></h4><p><img loading=lazy src=/images/helpner2/workflow_7.png alt=helpner-core></p><p>The most relevant step make the model available for everybody easily isn&rsquo;t properly mapped to a command, I have to do it manually for the moment üòÅ.</p><p>But lets assume the following command is already working (<em>for the moment, this is done using GitHub in the browser directly</em>):</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-console data-lang=console><span class=line><span class=cl><span class=go>spacy project run release
</span></span></span></code></pre></div><p>The previously created package is uploaded to github as a release (the <a href=https://github.com/plaguss/helpner-core/releases>releases</a> can be seen in the following page), including a small description of the model&rsquo;s accuracy, the weights and the necessary information to import the model just like any other spaCy model, after installation, it can be imported as usual in spaCy:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>spacy</span>
</span></span><span class=line><span class=cl><span class=n>nlp</span> <span class=o>=</span> <span class=n>spacy</span><span class=o>.</span><span class=n>load</span><span class=p>(</span><span class=s2>&#34;en_helpner_core&#34;</span><span class=p>)</span>
</span></span></code></pre></div><p>And just like we would do with any other spaCy model, we can pass text to it. As te expected input for the model are command line help messages, <a href=https://github.com/plaguss/helpner><code>helpner</code></a> helps dealing with the content directly on the shell.</p><h2 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h2><p>In this post we followed workflow outlined in <a href=https://github.com/plaguss/helpner-core><code>helpner-core</code></a>. We leveraged the power of spaCy project template to train, evaluate, and deploy a Named Entity Recognition model which can be <code>pip install</code>ed as a dependency to run as any spaCy model. Feels amazing to deal with an end to end NLP pipeline with just a few command line programs.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://plaguss.github.io/tags/nlp/>NLP</a></li><li><a href=https://plaguss.github.io/tags/ner/>NER</a></li><li><a href=https://plaguss.github.io/tags/spacy/>spaCy</a></li><li><a href=https://plaguss.github.io/tags/python/>Python</a></li></ul><nav class=paginav><a class=prev href=https://plaguss.github.io/blog/my-attempt-of-calling-rust-from-python/><span class=title>¬´ Prev</span><br><span>Pytokei, calling rust's tokei from python</span></a>
<a class=next href=https://plaguss.github.io/blog/a-ner-model-for-command-line-help-messages-part1/><span class=title>Next ¬ª</span><br><span>A NER Model for Command Line Help Messages (Part 1: The command line program)</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://plaguss.github.io/>Agus site</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>