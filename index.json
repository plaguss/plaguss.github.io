[{"content":"In this blog post I will walk through the steps I followed to start with rust, what resulted in pytokei, a python wrapper of tokei.\nâ¯ pytokei pytokei pytokei â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“ â”ƒ language â”ƒ Files â”ƒ Lines â”ƒ Code â”ƒ Comments â”ƒ Blanks â”ƒ â”¡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”© â”‚ Rust â”‚ 9 â”‚ 1011 â”‚ 846 â”‚ 49 â”‚ 116 â”‚ â”‚ Python â”‚ 5 â”‚ 568 â”‚ 436 â”‚ 13 â”‚ 119 â”‚ â”‚ Markdown â”‚ 11 â”‚ 423 â”‚ 123 â”‚ 179 â”‚ 121 â”‚ â”‚ Plain Text â”‚ 4 â”‚ 133 â”‚ 0 â”‚ 133 â”‚ 0 â”‚ â”‚ TOML â”‚ 3 â”‚ 75 â”‚ 59 â”‚ 6 â”‚ 10 â”‚ â”‚ YAML â”‚ 1 â”‚ 69 â”‚ 63 â”‚ 0 â”‚ 6 â”‚ â”‚ Makefile â”‚ 1 â”‚ 26 â”‚ 18 â”‚ 0 â”‚ 8 â”‚ â”‚ Dockerfile â”‚ 1 â”‚ 16 â”‚ 7 â”‚ 3 â”‚ 6 â”‚ â”‚ Shell â”‚ 3 â”‚ 12 â”‚ 12 â”‚ 0 â”‚ 0 â”‚ â”‚ Autoconf â”‚ 3 â”‚ 7 â”‚ 7 â”‚ 0 â”‚ 0 â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜ Intro After reading Rust\u0026rsquo;s book, I was looking for a excuse to build something in Rust. Just by taking a look at the book\u0026rsquo;s index, one starts to wonder what to begin with\u0026hellip;\nGiven that I\u0026rsquo;m comfortable enough with python, I tought it could be easy to write some bindings for a simple library already written in rust, so that I had some familiarity. By looking at PyO3 examples I found a simple library that could help as a simple guide: pyheck. Now its just a matter of finding a library to wrap.\nTo learn how to write bindings to rust from python its better for you to take a look at maturin and PyO3, here I just share my journey, but its not intented as a full guide to anything.\nPytokei Thinking about a good target library, I remembered a library I found tokei by chance some time ago. It seemed that exposing the public API to python should not be so hard.\nTokei From its readme:\nTokei is a program that displays statistics about your code. Tokei will show the number of files, total lines within those files and code, comments, and blanks grouped by language.\nIt is a simple command line program, but to expose to python what it does, we have to wrap as functions and or classes the contents of lib.rs, we don\u0026rsquo;t need anything about its CLI.\nDevelopment You can take a lookg at the development section of pytokei\u0026rsquo;s docs to get started. We just need to install maturin to create the package, and PyO3 to create the bindings.\nTo package the code we need a Cargo.toml for the rust and a pyproject.toml for python, all of which is controlled by maturin.\nSo\u0026hellip; I just want to mimic the behavior of tokei\u0026rsquo;s API, let\u0026rsquo;s see what we need to implement for that end.\nRust The public API of pytokei can be seen at the lib.rs file. Lets take a look at what we will have visible from python:\n#[pymodule] fn _pytokei(_py: Python, m: \u0026amp;PyModule) -\u0026gt; PyResult\u0026lt;()\u0026gt; { let version = env!(\u0026#34;CARGO_PKG_VERSION\u0026#34;).to_string(); m.add(\u0026#34;__version__\u0026#34;, version)?; m.add_class::\u0026lt;PyConfig\u0026gt;().unwrap(); m.add_class::\u0026lt;PyLanguages\u0026gt;().unwrap(); m.add_class::\u0026lt;PySort\u0026gt;().unwrap(); m.add_function(wrap_pyfunction!(sort_types, m)?)?; m.add_class::\u0026lt;PyCodeStats\u0026gt;().unwrap(); m.add_class::\u0026lt;PyReport\u0026gt;().unwrap(); m.add_class::\u0026lt;PyLanguageType\u0026gt;().unwrap(); m.add_class::\u0026lt;PyLanguage\u0026gt;().unwrap(); Ok(()) } We can read from the previous snippet that we have a module called _pytokei, along with some classes: PyConfig, PyLanguages\u0026hellip; and a single function sort_types. How do they look like in rust?\nTake a look at the implementation of the main class:\n#[pyclass(name = \u0026#34;Languages\u0026#34;)] pub struct PyLanguages { pub languages: Languages, } Its just a wrapper of the Languages struct from tokei, with the pyclass attribute from PyO3 to make it a python class, which for this case works much like a python decorator.\nLets see now some of the methods:\n#[pymethods] impl PyLanguages { #[new] pub fn new() -\u0026gt; Self { PyLanguages { languages: Languages::new(), } } pub fn get_statistics(\u0026amp;mut self, paths: Vec\u0026lt;String\u0026gt;, ignored: Vec\u0026lt;String\u0026gt;, config: \u0026amp;PyConfig) { let paths_: Vec\u0026lt;\u0026amp;str\u0026gt; = paths.iter().map(String::as_str).collect(); let paths_ = paths_.as_slice(); let ignored_: Vec\u0026lt;\u0026amp;str\u0026gt; = ignored.iter().map(String::as_str).collect(); let ignored_ = ignored_.as_slice(); self.languages .get_statistics(\u0026amp;paths_, \u0026amp;ignored_, \u0026amp;config.config) } pub fn total(\u0026amp;self) -\u0026gt; PyLanguage { PyLanguage { language: self.languages.total(), } } pub fn language_names(\u0026amp;self) -\u0026gt; PyResult\u0026lt;Vec\u0026lt;\u0026amp;str\u0026gt;\u0026gt; { let vec = self .languages .iter() .map(|(lang_type, _)| lang_type.name()) .collect(); Ok(vec) } ... It is just a matter of exposing the rust content to python, easy for this type of methods:\nnew: Initializes the PyLanguages struct, does the job of the python constructor (__init__).\nget_statistics: It just parses the arguments to pass them to the get_statistics method from rust, doesn\u0026rsquo;t return anything as the result is internally stored.\ntotal: Returns a PyLanguage in python with the result from an internal method.\nlanguage_names: Creates and returns a Vec of \u0026amp;str, which is automatically translated by PyO3 to a list of str in python.\nWe can practice rust one function at a time. For a simple case like this we just need to find how to expose the rust chunk of code to python, we don\u0026rsquo;t need to get into nothing much complicated.\nLet\u0026rsquo;s see now the what the python version looks like.\nPython API From the python side, given the library is simple enough, we just have to write the type stubs in a .pyi file: _pytokei.pyi for our case. There we can see all python code, as well as the docstrings, deployed at API docs.\nThe python interface to the PyLanguage struct from rust can be seen here:\nclass Languages: \u0026#34;\u0026#34;\u0026#34;A class representing a list of languages counted in the provided directory. See [`LanguageType.list`](language_type.md) Examples -------- ```python \u0026gt;\u0026gt;\u0026gt; from pytokei import Languages \u0026gt;\u0026gt;\u0026gt; langs = Languages() \u0026gt;\u0026gt;\u0026gt; langs Languages() ``` References ---------- [Languages implementation](https://docs.rs/tokei/latest/tokei/struct.Languages.html#impl) \u0026#34;\u0026#34;\u0026#34; def __init__(self) -\u0026gt; None: ... def get_statistics( self, paths: list[str], ignored: list[str], config: Config ) -\u0026gt; None: \u0026#34;\u0026#34;\u0026#34;Populates the Languages struct with statistics about languages provided by Language. Takes a list of of paths (as str) to recursively traverse, paths can be relative, absolute or glob paths. A second list of paths (as str) to ignore, these strings use the `.gitignore` syntax, such as `target` or `**/*.bk`. Parameters ---------- paths : list[str] List of files to traverse. It may be a single directory. ignored : list[str] List of files to ignore. If you don\u0026#39;t want anything ignored, just pass `[\u0026#34;ignored\u0026#34;]`. config : Config Config instance. If you dont have any preferences, just pass `Config`. \u0026#34;\u0026#34;\u0026#34; def total(self) -\u0026gt; Language: \u0026#34;\u0026#34;\u0026#34;Summary of the Languages struct.\u0026#34;\u0026#34;\u0026#34; def language_names(self) -\u0026gt; Optional[list[str]]: \u0026#34;\u0026#34;\u0026#34;Returns the list of language names, if any was found.\u0026#34;\u0026#34;\u0026#34; ... It doesn\u0026rsquo;t contain the actual implementation, just the skeleton, enough for an IDE to help us with the autocompletion.\nLets see an example running pytokei against pip:\n$ python \u0026gt;\u0026gt;\u0026gt; import pytokei \u0026gt;\u0026gt;\u0026gt; from rich import print \u0026gt;\u0026gt;\u0026gt; langs = pytokei.Languages() \u0026gt;\u0026gt;\u0026gt; langs.get_statistics([\u0026#34;.\u0026#34;], [\u0026#34;nothing\u0026#34;], pytokei.Config()) \u0026gt;\u0026gt;\u0026gt; print(langs.report_compact_plain()) { \u0026#39;TOML\u0026#39;: {\u0026#39;blanks\u0026#39;: 14, \u0026#39;comments\u0026#39;: 15, \u0026#39;files\u0026#39;: 17, \u0026#39;lines\u0026#39;: 162, \u0026#39;code\u0026#39;: 133}, \u0026#39;ReStructuredText\u0026#39;: {\u0026#39;comments\u0026#39;: 0, \u0026#39;files\u0026#39;: 78, \u0026#39;code\u0026#39;: 7126, \u0026#39;lines\u0026#39;: 9702, \u0026#39;blanks\u0026#39;: 2576}, \u0026#39;C\u0026#39;: {\u0026#39;code\u0026#39;: 0, \u0026#39;comments\u0026#39;: 0, \u0026#39;blanks\u0026#39;: 0, \u0026#39;files\u0026#39;: 1, \u0026#39;lines\u0026#39;: 0}, \u0026#39;Markdown\u0026#39;: {\u0026#39;blanks\u0026#39;: 861, \u0026#39;files\u0026#39;: 27, \u0026#39;comments\u0026#39;: 2243, \u0026#39;lines\u0026#39;: 3408, \u0026#39;code\u0026#39;: 304}, \u0026#39;PowerShell\u0026#39;: {\u0026#39;files\u0026#39;: 1, \u0026#39;lines\u0026#39;: 74, \u0026#39;comments\u0026#39;: 2, \u0026#39;blanks\u0026#39;: 11, \u0026#39;code\u0026#39;: 61}, \u0026#39;Python\u0026#39;: {\u0026#39;lines\u0026#39;: 220272, \u0026#39;files\u0026#39;: 709, \u0026#39;code\u0026#39;: 188695, \u0026#39;blanks\u0026#39;: 22068, \u0026#39;comments\u0026#39;: 9509}, \u0026#39;Autoconf\u0026#39;: {\u0026#39;comments\u0026#39;: 0, \u0026#39;lines\u0026#39;: 50, \u0026#39;code\u0026#39;: 44, \u0026#39;blanks\u0026#39;: 6, \u0026#39;files\u0026#39;: 10}, \u0026#39;Plain Text\u0026#39;: {\u0026#39;lines\u0026#39;: 1293, \u0026#39;files\u0026#39;: 18, \u0026#39;blanks\u0026#39;: 96, \u0026#39;code\u0026#39;: 0, \u0026#39;comments\u0026#39;: 1197}, \u0026#39;HTML\u0026#39;: {\u0026#39;code\u0026#39;: 77, \u0026#39;comments\u0026#39;: 0, \u0026#39;blanks\u0026#39;: 0, \u0026#39;lines\u0026#39;: 77, \u0026#39;files\u0026#39;: 11} } We are obtaining the statistics from the current working directory (in my case I was in the folder containing pip), we filter \u0026ldquo;nothing\u0026rdquo; (this is just a placeholder, no file or folder is called like that, so it works), and use the default Config, and we print the compact report, meaning we print the statistics at the language level.\nYou can see the equivalent example from tokei at docs.rs.\nPython CLI pytokei also exposes a python API and a cli which shows the results in the console similar to what tokei does, but using rich. This is just an extra to mimic to obtain a similar command line interface to the one tokei gives us, but there is no more rust involved in this step.\nThe following block shows the result of running the program from the console against pip:\nâ¯ pytokei pip pip â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“ â”ƒ language â”ƒ Files â”ƒ Lines â”ƒ Code â”ƒ Comments â”ƒ Blanks â”ƒ â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”© â”‚ Python â”‚ 709 â”‚ 220272 â”‚ 188695 â”‚ 9509 â”‚ 22068 â”‚ â”‚ ReStructuredText â”‚ 78 â”‚ 9702 â”‚ 7126 â”‚ 0 â”‚ 2576 â”‚ â”‚ Markdown â”‚ 27 â”‚ 3408 â”‚ 304 â”‚ 2243 â”‚ 861 â”‚ â”‚ Plain Text â”‚ 18 â”‚ 1293 â”‚ 0 â”‚ 1197 â”‚ 96 â”‚ â”‚ TOML â”‚ 17 â”‚ 162 â”‚ 133 â”‚ 15 â”‚ 14 â”‚ â”‚ HTML â”‚ 11 â”‚ 77 â”‚ 77 â”‚ 0 â”‚ 0 â”‚ â”‚ PowerShell â”‚ 1 â”‚ 74 â”‚ 61 â”‚ 2 â”‚ 11 â”‚ â”‚ Autoconf â”‚ 10 â”‚ 50 â”‚ 44 â”‚ 0 â”‚ 6 â”‚ â”‚ C â”‚ 1 â”‚ 0 â”‚ 0 â”‚ 0 â”‚ 0 â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜ Conclusion Its just a simple library, but it help me gain some confidence not only writing rust, but also reading an proper rust project, how to wrap it in python using PyO3 and maturin, and upload the final result to PyPI.\nYou can visit the github repository at pytokei as well as the documentation.\n","permalink":"https://plaguss.github.io/blog/my-attempt-of-calling-rust-from-python/","summary":"In this blog post I will walk through the steps I followed to start with rust, what resulted in pytokei, a python wrapper of tokei.\nâ¯ pytokei pytokei pytokei â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“ â”ƒ language â”ƒ Files â”ƒ Lines â”ƒ Code â”ƒ Comments â”ƒ Blanks â”ƒ â”¡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”© â”‚ Rust â”‚ 9 â”‚ 1011 â”‚ 846 â”‚ 49 â”‚ 116 â”‚ â”‚ Python â”‚ 5 â”‚ 568 â”‚ 436 â”‚ 13 â”‚ 119 â”‚ â”‚ Markdown â”‚ 11 â”‚ 423 â”‚ 123 â”‚ 179 â”‚ 121 â”‚ â”‚ Plain Text â”‚ 4 â”‚ 133 â”‚ 0 â”‚ 133 â”‚ 0 â”‚ â”‚ TOML â”‚ 3 â”‚ 75 â”‚ 59 â”‚ 6 â”‚ 10 â”‚ â”‚ YAML â”‚ 1 â”‚ 69 â”‚ 63 â”‚ 0 â”‚ 6 â”‚ â”‚ Makefile â”‚ 1 â”‚ 26 â”‚ 18 â”‚ 0 â”‚ 8 â”‚ â”‚ Dockerfile â”‚ 1 â”‚ 16 â”‚ 7 â”‚ 3 â”‚ 6 â”‚ â”‚ Shell â”‚ 3 â”‚ 12 â”‚ 12 â”‚ 0 â”‚ 0 â”‚ â”‚ Autoconf â”‚ 3 â”‚ 7 â”‚ 7 â”‚ 0 â”‚ 0 â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜ Intro After reading Rust\u0026rsquo;s book, I was looking for a excuse to build something in Rust.","title":"Pytokei, calling rust's tokei from python"},{"content":"Lets continue our journey with helpner-core, the spaCy project that does all the magic behind helpner.\nThis repository contains a spaCy template for a NER model, and allows to build the end-to-end spaCy workflow from the spacy project cli command manager. In short, spaCy projects present End-to-end NLP workflows from prototype to production.\nWe will visit the different steps of the whole workflow following the commands that involve the pipeline.\nhelpner-core workflow All this process is stored in the project.yml file, which contains the single source of truth for the project.\nDataset creation The first step is the dataset creation. I could have gone down the path of gathering a different number of CLI help messages and start labelling them, but it could easily get a very tedious task, and as the task allowed it, I started implementing a synthetic data generator (I will present cli-help-maker in a different post).\nLets introduce the first command:\nspacy project run create-dataset Running this command calls the program cli-help-maker, with a dataset.yaml file which contains the arguments needed for the program. This simplifies the versioning of the different data sets, we just have to store the yaml of any given version, all the relevant information to create a dataset is contained in a single file. Other than that the dataset contains random data with the layout and content we want the model to capture.\nThis command returns two files:\ndataset.jsonl: A jsonl file, where each line corresponds to a sample, that contains message and the annotations. {\u0026#34;message\u0026#34;:\u0026#34;Usage: \\n chedlock digynia borofluoric begirdle underbeadle [-d] [CONGRUENTLY...] [ACROBATISM... [ZONOID...]]\\n\\nFusillade preventively hogyard approachles.\\n\\nCommands:\\n digynia\\n...\u0026#34;, \u0026#34;annotations\u0026#34;: [[\u0026#34;CMD\u0026#34;,21,28],[\u0026#34;CMD\u0026#34;, 29,40]...] } {\u0026#34;message\u0026#34;: \u0026#34;usage: \\n sterelmintha hearth --hydromorphic-beethovian -g --titanical -e ODONTOLCATE-\\n...\u0026#34;, \u0026#34;annotations\u0026#34;:[[\u0026#34;CMD\u0026#34;,25,31],[\u0026#34;OPT\u0026#34;,32,57]...]} ... arguments.jsonl: Another jsonl file not used for during the program. We keep in this file the arguments that were used to generate each CLI help message, just for further analysis, to know what to expect from each dataset. {\u0026#34;indent_spaces\u0026#34;:4.0,\u0026#34;total_width\u0026#34;:120.0,...} {\u0026#34;indent_spaces\u0026#34;:4.0,\u0026#34;total_width\u0026#34;:78.0,...} Now we are ready to prepare the dataset for the training process.\nData workflow The process is divided in two different worflows just to outline the fact that the first part corresponds to data preparation, training and evaluation, while the next part deals with packaging related processes.\nData preparation This step involves two different scripts:\nsplit.py, which is run by the following program: spacy project run split This command simply splits the file in two different files dataset_train.jsonl and dataset_dev.jsonl with a random partition of 80/20. Given the dataset is generated randomly, there is no need for more complicated strategies.\nconvert.py, run by the following program: spacy project run convert It transforms the message in each row of the jsonl file to a serializable format: DocBin, to obtain two new files dataset_train.spacy and dataset_dev.spacy.\nIts time for some action!\nTraining The following program starts the training process:\nspacy project run train This command reads all the necessary info from the config files (you can read more in the spaCy training docs), which correspond to the NER components, CPU hardware and optimized for efficiency.\nIts created from the spaCy defaults, there is no need to change anything during this step. Given the model is just a proof of concept, and the dataset used is small enough, we can just train in our personal computer without a GPU, it just needs some minutes to finish the training process.\nAfter this step has finished, we are left with a directory containing our named entity recognition model.\nEvaluation After running the benchmark accuracy command we can inspect the model results on the development dataset. There should exist a different dataset to test the results, but for simplicity, the results are reported on this dataset, and the model is finally run against some help messages from real CLI apps (watch the helpner readme).\nspacy project run evaluate The output file of the command is a metrics.json file that contains information about the model\u0026rsquo;s performance (this content can be transferred to a README file easily as we will see later). The following table presents the information per entity, NER per type table:\nP R F CMD 98.25 99.96 99.10 ARG 94.79 89.97 92.32 OPT 98.88 98.96 98.92 The first thing to note is that the results seem to be really good for such a simple model with no special meaning besides the layout of the entities. The smallest value corresponds to the recall (R) value of ARG entities (89.97), the element that tends to get more confused, while the highest corresponds to the recall of CMD.\nBut, this results must be taken carefully, the dataset in which the model was trained is totaly synthetic, and the results may not be as good as expected in real data. It just shows that the model is able to learn from the data it was fed with.\nPackaging workflow Up to this point, we run the enabled workflows in project.yml file, which correspond to:\nspacy project run all Those are related to the data processing, model training and evaluation. A second workflow (more a subsequent set of commands) is for house keeping related tasks:\nPackaging The first of these commands is the package. SpaCy easily allows to generate a package automatically from our model, just by running a command, so it can be easily installed and loaded afterwards:\nspacy project run package Just place the contents generated somewhere accessible to pip, and you are ready to pip install the model to be used with spaCy. It even comes with a README.md file generated from the metrics.json file obtained, so the relevant content is autoexplained.\nFollowing the spaCy approach to model storage with spaCy models, the trained models are stored in the releases of the repository, and helpner deals with the installation process via pip install.\nReadme One last command and we are finished!\nSpaCy templates come with another magic command to generate a pretty README.md for your project: spacy project document. I wanted to add some more content automatically to this README file, and with a little script and the help of wasabi (a Explosion library which helps with console printing, but also with Markdown rendering), its as simple as it gets:\nspacy project run readme This command generates the readme for the helpner-core repository automatically, adding to the original generated README some additional metrics, like information from the dataset used for the current version, which seemed interesting enough to be added.\nDeployment The most relevant step make the model available for everybody easily isn\u0026rsquo;t properly mapped to a command, I have to do it manually for the moment ğŸ˜.\nBut lets assume the following command is already working (for the moment, this is done using GitHub in the browser directly):\nspacy project run release The previously created package is uploaded to github as a release (the releases can be seen in the following page), including a small description of the model\u0026rsquo;s accuracy, the weights and the necessary information to import the model just like any other spaCy model, after installation, it can be imported as usual in spaCy:\nimport spacy nlp = spacy.load(\u0026#34;en_helpner_core\u0026#34;) And just like we would do with any other spaCy model, we can pass text to it. As te expected input for the model are command line help messages, helpner helps dealing with the content directly on the shell.\nConclusion In this post we followed workflow outlined in helpner-core. We leveraged the power of spaCy project template to train, evaluate, and deploy a Named Entity Recognition model which can be pip installed as a dependency to run as any spaCy model. Feels amazing to deal with an end to end NLP pipeline with just a few command line programs.\n","permalink":"https://plaguss.github.io/blog/a-ner-model-for-command-line-help-messages-part2/","summary":"Lets continue our journey with helpner-core, the spaCy project that does all the magic behind helpner.\nThis repository contains a spaCy template for a NER model, and allows to build the end-to-end spaCy workflow from the spacy project cli command manager. In short, spaCy projects present End-to-end NLP workflows from prototype to production.\nWe will visit the different steps of the whole workflow following the commands that involve the pipeline.","title":"A NER Model for Command Line Help Messages (Part 2: spaCy projects to the rescue)"},{"content":"In this 3 part series I will tell the journey of creating a program to detect the different components/entities of a command line program\u0026rsquo;s help message. This post will start by looking at the final product helpner, a python program that can be installed from PyPI, the second will tell about the spaCy NLP workflow and finally we will take a look at the data that feeds spaCy\u0026rsquo;s final model.\nThe post assumes some python knowledge, like how to install a library from PyPI, and some familiarity with spaCy.\nThe following figure shows the architecture of helpner, each piece is represented by a different github repository. Highlighted in yellow at the bottom its the piece corresponding to helpner.\nWhat led me to start this project? I wanted an NLP project to put into practice the spaCy facilities. If possible, the model should be ready to use by an end user without developing a website for it for simplicity. Independently, I found docopt by chance exploring python CLI libraries. As it turns out, this library, and its maintained fork (docopt-ng) can generate a CLI program by parsing a \u0026ldquo;properly written\u0026rdquo; help message (visit the previous link to see an example). This same idea seemed like a good opportunity.\nCan we solve this using a Named Entity Recognition model? \u0026hellip;I don\u0026rsquo;t care if its not the best approach\nLets try to write a CLI program that can take a help message from another CLI program, and find the different elements or entities (commands, arguments and options) which conform it. It turns out that in around 200 lines of code, we can have a promising first version ğŸ˜„. Of course, this is not that simple, but with spaCy it feels like it ğŸ‘Œ.\nEnter helpner Lets see how helpner works. The installation consists of two steps. First, install using pip as usual preferably inside a venv. (It should be possible to install it using pipx, but I haven\u0026rsquo;t tried it yet):\n$ pip install helpner This should have downloaded the library, but as of this moment its incomplete, we still have to download the model itself. For this, a handy command is supplied (visit the README.md for more information):\n$ helpner download This two step process should be familiar for those who have already used spaCy. By using this approach it allows to split the development of the model from the use we make of it. We could update the model in any way (we could for example retrain the model with different data, or modify the optimizer used), and we would only need to update the model (running again the download command). It applies the same for the library, we could add more functionality without changing the inner model.\nWe are already in position to use helpner ğŸ’¥, lets see one of the examples from the docs, how to highlight the entities of a help message (this was the first use that came to mind):\nflit install --help | helpner highlight For those who don\u0026rsquo;t know flit, its a command line program that simplifies packaging python modules. The example shows the help message of one of its subcommands, flit install. From the legend we see that the possible elements or entities are CMD (commands or subcommands, which in this case depend on flit directly), ARG (positional arguments, which in this case don\u0026rsquo;t exist) and OPT (optional arguments, which correspond to all the elements preceded by a single or double dash, are correctly predicted). But it calls the attention some random words highlighted as if they were CMD entities, which are clearly misplaced. It is far from perfect, but I consider it a success anyway, the results seem promising enough!\nWhat happens underneath? what we did was send the help message to the spaCy model, and get the predictions:\nâ¯ flit install --help | helpner parse --no-json { \u0026#39;install\u0026#39;: (\u0026#39;CMD\u0026#39;, 12, 19), \u0026#39;[-h]\u0026#39;: (\u0026#39;OPT\u0026#39;, 20, 24), \u0026#39;[-s]\u0026#39;: (\u0026#39;OPT\u0026#39;, 25, 29), \u0026#39;[--pth-file]\u0026#39;: (\u0026#39;OPT\u0026#39;, 30, 42), \u0026#39;[--user]\u0026#39;: (\u0026#39;OPT\u0026#39;, 43, 51), \u0026#39;[--env]\u0026#39;: (\u0026#39;OPT\u0026#39;, 52, 59), \u0026#39;[--python PYTHON]\u0026#39;: (\u0026#39;OPT\u0026#39;, 60, 77), \u0026#39;[--deps {all,production,develop,none}]\u0026#39;: (\u0026#39;OPT\u0026#39;, 98, 136), \u0026#39;[--only-deps]\u0026#39;: (\u0026#39;OPT\u0026#39;, 137, 150), \u0026#39;[--extras EXTRAS]\u0026#39;: (\u0026#39;OPT\u0026#39;, 171, 188), \u0026#39;-h, --help\u0026#39;: (\u0026#39;OPT\u0026#39;, 201, 211), \u0026#39;exit\u0026#39;: (\u0026#39;CMD\u0026#39;, 250, 254), \u0026#39;-s, --symlink\u0026#39;: (\u0026#39;OPT\u0026#39;, 257, 270), \u0026#39;package\u0026#39;: (\u0026#39;CMD\u0026#39;, 298, 305), \u0026#39;--pth-file\u0026#39;: (\u0026#39;OPT\u0026#39;, 373, 383), \u0026#39;module\u0026#39;: (\u0026#39;CMD\u0026#39;, 417, 423), \u0026#39;/\u0026#39;: (\u0026#39;CMD\u0026#39;, 423, 424), \u0026#39;--user\u0026#39;: (\u0026#39;OPT\u0026#39;, 497, 503), \u0026#39;local\u0026#39;: (\u0026#39;CMD\u0026#39;, 529, 534), \u0026#39;--env\u0026#39;: (\u0026#39;OPT\u0026#39;, 612, 617), \u0026#39;--python PYTHON\u0026#39;: (\u0026#39;OPT\u0026#39;, 749, 764), \u0026#39;--deps {all,production,develop,none}\u0026#39;: (\u0026#39;OPT\u0026#39;, 862, 898), \u0026#39;--only-deps\u0026#39;: (\u0026#39;OPT\u0026#39;, 1074, 1085), \u0026#39;--extras EXTRAS\u0026#39;: (\u0026#39;OPT\u0026#39;, 1192, 1207), \u0026#39;the\u0026#39;: (\u0026#39;CMD\u0026#39;, 1313, 1316), \u0026#39;ones\u0026#39;: (\u0026#39;CMD\u0026#39;, 1317, 1321), \u0026#39;implied\u0026#39;: (\u0026#39;CMD\u0026#39;, 1322, 1329), \u0026#39;by\u0026#39;: (\u0026#39;CMD\u0026#39;, 1330, 1332), \u0026#39;be\u0026#39;: (\u0026#39;CMD\u0026#39;, 1382, 1384), \u0026#39;useful\u0026#39;: (\u0026#39;CMD\u0026#39;, 1385, 1391) } This output has all the necessary information to inform rich. The keys in the dict correspond to the elements found/predicted, and the values contain the entity, start and end position of the substrings. With this information, we can make use of rich to add some color to the console.\nOf course, there are multiple errors (and this is an example that seems relatively right), the model cannot be better than the data it was fed with. In a posterior post we will see how the data powering this model is obtained.\n","permalink":"https://plaguss.github.io/blog/a-ner-model-for-command-line-help-messages-part1/","summary":"In this 3 part series I will tell the journey of creating a program to detect the different components/entities of a command line program\u0026rsquo;s help message. This post will start by looking at the final product helpner, a python program that can be installed from PyPI, the second will tell about the spaCy NLP workflow and finally we will take a look at the data that feeds spaCy\u0026rsquo;s final model.","title":"A NER Model for Command Line Help Messages (Part 1: The command line program)"},{"content":"","permalink":"https://plaguss.github.io/archive/","summary":"archive","title":"Archive"}]